Here's a README.md file that explains the entire process of Change Data Capture (CDC) using AWS Data Migration Service (DMS), Glue, Spark, Lambda, and CloudWatch Logs.

# Change Data Capture using AWS DMS, Glue, Spark, Lambda, and CloudWatch Logs

This repository contains code and instructions to implement Change Data Capture (CDC) using a combination of AWS services including AWS Data Migration Service (DMS), AWS Glue, Apache Spark, AWS Lambda, and AWS CloudWatch Logs. The solution captures changes from a source database, processes them using Spark, and stores the results in an S3 bucket.

## Architecture Overview

1. **AWS DMS**: Captures changes from the source database and stores them in an S3 bucket.
2. **AWS Glue**: Orchestrates and runs Spark jobs to process the captured changes.
3. **Apache Spark**: Processes the data, applying updates, inserts, and deletions.
4. **AWS Lambda**: Triggers Glue jobs based on S3 events.
5. **AWS CloudWatch Logs**: Monitors and logs the process for auditing and troubleshooting.

## Prerequisites

1. **AWS Account**: An active AWS account.
2. **AWS CLI**: Installed and configured with appropriate permissions.
3. **Apache Spark**: Installed locally or on an EMR cluster.
4. **Python 3.x**: Installed on your local machine.
5. **Boto3**: AWS SDK for Python, installed via `pip install boto3`.
6. **PySpark**: Installed via `pip install pyspark`.

## Setup Steps

### 1. AWS DMS

1. **Create a replication instance** in AWS DMS.
2. **Create source and target endpoints**.
3. **Create a replication task** to capture changes from the source database and store them in an S3 bucket.

### 2. AWS S3

1. **Create an S3 bucket** to store the CDC data.
2. **Update the DMS replication task** to use this bucket as the target.

### 3. AWS Glue

1. **Create a Glue job** to process the CDC data using the provided Spark script.
2. **Set up a Glue crawler** to create a data catalog for the processed data.

### 4. AWS Lambda

1. **Create a Lambda function** to trigger the Glue job on S3 events.
2. **Set up S3 event notifications** to invoke the Lambda function when new files are added to the bucket.

### 5. AWS CloudWatch Logs

1. **Create log groups and log streams** to capture logs from Lambda and Glue jobs.
2. **Set up CloudWatch Alarms** to monitor for failures and performance issues.

## Usage

### Spark CDC Script

The provided Spark script processes CDC data stored in S3. It performs the following steps:

1. Reads data from the S3 bucket.
2. Differentiates between initial load files and update files.
3. Applies updates, inserts, and deletions based on the CDC actions.
4. Writes the final processed data back to the S3 bucket.

### Lambda Function

The Lambda function triggers the Glue job whenever a new file is added to the S3 bucket. It retrieves the bucket name and file name from the event, and passes them as arguments to the Glue job.

### Detailed Code Explanation

#### Lambda Function

```python
import json
import boto3

def lambda_handler(event, context):
    # Extract bucket name and file name from the event
    bucketName = event["Records"][0]["s3"]["bucket"]["name"]
    fileName = event["Records"][0]["s3"]["object"]["key"]

    print(bucketName, fileName)

    # Initialize Glue client
    glue = boto3.client('glue')

    # Start Glue job with arguments
    response = glue.start_job_run(
        JobName='glueCDC-pyspark',
        Arguments={
            '--s3_target_path_key': fileName,
            '--s3_target_path_bucket': bucketName
        }
    )

    return {
        'statusCode': 200,
        'body': json.dumps('Hello from Lambda!')
    }
from awsglue.utils import getResolvedOptions
import sys
from pyspark.sql.functions import when
from pyspark.sql import SparkSession

# Parse arguments passed to the Glue job
args = getResolvedOptions(sys.argv, ['s3_target_path_key', 's3_target_path_bucket'])
bucket = args['s3_target_path_bucket']
fileName = args['s3_target_path_key']

print(bucket, fileName)

# Initialize Spark session
spark = SparkSession.builder.appName("CDC").getOrCreate()

# Construct S3 file paths
inputFilePath = f"s3a://{bucket}/{fileName}"
finalFilePath = f"s3a://cdc-ouput-pyspark/output"

# Initial data load
if "LOAD" in fileName:
    fldf = spark.read.csv(inputFilePath, header=False)
    fldf = fldf.withColumnRenamed("_c0", "id").withColumnRenamed("_c1", "FullName").withColumnRenamed("_c2", "City")
    fldf.write.mode("overwrite").csv(finalFilePath, header=True)
else:
    # Update handling
    udf = spark.read.csv(inputFilePath, header=False)
    udf = udf.withColumnRenamed("_c0", "action").withColumnRenamed("_c1", "id").withColumnRenamed("_c2", "FullName").withColumnRenamed("_c3", "City")
    ffdf = spark.read.csv(finalFilePath, header=True)
    
    # Apply updates, inserts, and deletions
    udf_local = udf.collect()
    for row in udf_local:
        action = row["action"]
        row_id = row["id"]
        full_name = row["FullName"]
        city = row["City"]
        
        if action == 'U':
            ffdf = ffdf.withColumn("FullName", when(ffdf["id"] == row_id, full_name).otherwise(ffdf["FullName"]))
            ffdf = ffdf.withColumn("City", when(ffdf["id"] == row_id, city).otherwise(ffdf["City"]))
        
        elif action == 'I':
            new_row = [(row_id, full_name, city)]
            columns = ['id', 'FullName', 'City']
            newdf = spark.createDataFrame(new_row, columns)
            ffdf = ffdf.union(newdf)
        
        elif action == 'D':
            ffdf = ffdf.filter(ffdf.id != row_id)
    
    # Write final data
    ffdf.write.mode("overwrite").csv(finalFilePath, header=True)
```
## Monitoring and Logging
CloudWatch Logs: Logs from the Lambda function and Glue job are sent to CloudWatch Logs for monitoring and troubleshooting.
CloudWatch Alarms: Set up alarms to monitor for job failures or performance issues.

## Conclusion
This solution provides a robust way to handle CDC using a combination of AWS services. It ensures that changes in the source database are accurately captured and applied to the target dataset stored in S3, enabling near real-time data synchronization.